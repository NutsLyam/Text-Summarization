{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pymorphy2\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import bisect\n",
    "from collections import namedtuple,defaultdict\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "#MS SQL \n",
    "import pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MS SQL\n",
    "server = 'NUTS' \n",
    "#database = 'ThesisMK' \n",
    "database = 'MK_news_1000'\n",
    "username = '' \n",
    "password = '' \n",
    "cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "cur_sql = cnxn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "stopWords = stopwords.words('russian')\n",
    "stopWords.extend(['что', 'это', 'так', 'вот','ещё', 'свой' , 'быть','й', 'как', 'в', '—', 'к', 'на','свой', 'который', 'е','кстати', 'также'])\n",
    "\n",
    "def parse_text(text):\n",
    "    #print(type(text))\n",
    "    #words = (word for word in re.split('\\W+', text) if (len(word) >0 )&( analyzer.normal_forms(word)[0].lower() not in stopWords ))\n",
    "    words = (word for word in re.split('\\W+', text) if (len(word) >0 )&(re.match(r'(\\D+$)',word)is not None)&( analyzer.normal_forms(word)[0].lower() not in stopWords ))\n",
    "    \n",
    "    lexems = (analyzer.normal_forms(word)[0] for word in words)\n",
    "    \n",
    "    return list(lexems)\n",
    "\n",
    "\n",
    "class InvertedIndex:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.dict = defaultdict(list)\n",
    "        self.texts = dict()\n",
    "        \n",
    "    def add_document(self, text, doc_id):\n",
    "        self.texts[doc_id] = text\n",
    "        words = parse_text(text)\n",
    "        \n",
    "        \n",
    "        word_to_entry = defaultdict(lambda: [])\n",
    "        \n",
    "        for pos, word in enumerate(words):\n",
    "            \n",
    "            doc_entry = word_to_entry[word]\n",
    "            doc_entry.append(pos)\n",
    "            \n",
    "        for word, positions in word_to_entry.items():\n",
    "            postings = self.dict[word]\n",
    "            entry = DocEntry(doc_id, positions)\n",
    "            \n",
    "            i = bisect.bisect_left(postings, entry)\n",
    "        \n",
    "            postings.insert(i, entry)\n",
    "            \n",
    "    def get_postings(self, word):\n",
    "        return self.dict[word]\n",
    "    \n",
    "\n",
    "        \n",
    "\"\"\"Подсчет векторного произведения\"\"\"        \n",
    "def mult_single(x,y):\n",
    "    #conditions x>0 y>o\n",
    "    return x*y\n",
    "\n",
    "def mult_vect(x,y):\n",
    "    mult = np.vectorize(mult_single)\n",
    "    return np.sum(mult(x,y))\n",
    "\n",
    "\"\"\"Dkl\"\"\"\n",
    "\n",
    "def dkl_single(x,y):\n",
    "    if x>0 and y>0:\n",
    "     \n",
    "        return x*math.log2(x/y)\n",
    "    elif x==0: return 0.0\n",
    "    else: return math.inf\n",
    "\n",
    "def dkl(x,y):\n",
    "    sum_dkl = np.vectorize(dkl_single)\n",
    "    \n",
    "    return np.sum(sum_dkl(x,y))\n",
    "\n",
    "def div_matr(D, n, m,py_x,py_t) :\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            Div[i,j] = dkl(py_x[:,i],py_t.transpose()[:,j]) \n",
    "            \n",
    "    return Div\n",
    "    \n",
    "\"\"\"Djs\"\"\"\n",
    "def djs(x,y):\n",
    "    return 0.5 * dkl(x, (x+y)/2) + 0.5* dkl(y, (x+y)/2)\n",
    "\n",
    "def djs_vect(Djs,m,pt_x,pt_x_new ):\n",
    "    for i in range(m):\n",
    "        Djs[i]= djs(pt_x[:,i], pt_x_new[:,i])\n",
    "        \n",
    "        #Djs[i]= djs(pt_x[i,:], pt_x_new[i,:])\n",
    "    return Djs\n",
    "\n",
    "\"\"\"P(t)\"\"\"\n",
    "def f_pt(pt,m,px,pt_x):\n",
    "    for i in range(m):\n",
    "        pt[i] = mult_vect(px, pt_x[i,:])\n",
    "    return pt\n",
    "\n",
    "\"\"\"P(y|t)\"\"\"\n",
    "def f_py_t(py_t, m,n,pt_x,pxy,pt):\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            py_t[i,j] = mult_vect(pt_x[i,:],pxy[:,j])/pt[i]        \n",
    "    return py_t\n",
    "\"\"\"Выводит список из  n элементов, соответсвующих  \"\"\"\n",
    "def main_words(n, r):\n",
    "    l = []\n",
    "\n",
    "    for i in range(n):\n",
    "        l.append(r.argmax())\n",
    "        r[r.argmax()] = 0\n",
    "    return l\n",
    "\n",
    "\n",
    "        \n",
    "#show all numbers of docs from  cl,   \n",
    "def show_text_docs(cl_num):\n",
    "    show_main_words(cl_num)\n",
    "    print(\"\\n\")\n",
    "    docs = doc_clast[cl_num] # список документов кластера №cl_num\n",
    "    for num in docs:\n",
    "        print(\"№ документа: \" + str(num) + \" ТЕКСТ \"+ str(x[num]))\n",
    "    \n",
    "    \n",
    "    \n",
    "def foo_z(pt_x):\n",
    "    z = pt_x.sum(axis=0)\n",
    "    return z\n",
    "\n",
    "\n",
    "#w_d список документов со словом w\n",
    "#d_cl список документов из кластера t \n",
    "\n",
    "#w_d,  d_cl\n",
    "def f_N11( w_d,d_cl,n):\n",
    "    k= 0\n",
    "    for d in range(n):\n",
    "        if (d in d_cl )& (d in w_d):\n",
    "            k+=1\n",
    "    return k\n",
    "#not w_d, w_cl\n",
    "def f_N01(w_d,w_cl,n):\n",
    "    k= 0\n",
    "    for d in range(n):\n",
    "        if (d in w_cl )& (d not in w_d):\n",
    "            k+=1\n",
    "    return k\n",
    "#not w_d, not w_cl\n",
    "def f_N00( w_d,w_cl,n):\n",
    "    k= 0\n",
    "    for d in range(n):\n",
    "        if (d not in w_cl )& (d not in w_d):\n",
    "            k+=1\n",
    "    return k\n",
    "\n",
    "#w_d,   not w_cl\n",
    "def f_N10( w_d,w_cl,n):\n",
    "    k= 0\n",
    "    for d in range(n):\n",
    "        if (d not in w_cl )& (d in w_d):\n",
    "            k+=1\n",
    "    return k\n",
    "\n",
    "def mutual_information(t):\n",
    "    MI = []\n",
    "    for word in tqdm(word_dict.items()):\n",
    "        w  = word[1]\n",
    "        \n",
    "        w_d=[]\n",
    "        for doc_id,pos in index.dict.get(w):\n",
    "            w_d.append(doc_id)\n",
    "        N = len(x)\n",
    "        N11 = f_N11(w_d,doc_clast.get(t),len(x))\n",
    "        N1 = len(w_d)\n",
    "        N10 = f_N10(w_d,doc_clast.get(t),len(x))\n",
    "        N0 = N - N1\n",
    "        #print(N, N1, N0)\n",
    "        N01 = f_N01(w_d,doc_clast.get(t),len(x))\n",
    "        N00 = f_N00(w_d,doc_clast.get(t),len(x))\n",
    "        if N10==0: N10=0.0001\n",
    "        if N01==0: N01=0.0001\n",
    "        if N11==0: N11=0.0001\n",
    "        if N00==0: N00=0.0001\n",
    "   \n",
    "        #print(N11, N01, N10, N00)\n",
    "        I = N11/N * math.log2(N*N11/(N1*N1)) + N01/N*math.log2(N * N01/(N0*N1)) +N10/N*math.log2(N * N10/(N0*N1))+N00/N*math.log2(N * N00/(N0*N0))\n",
    "        MI.append(I)\n",
    "    return MI\n",
    "    \n",
    "def main_words(n, r):\n",
    "    l = []\n",
    "    \n",
    "    for i in range(n): \n",
    "        maxi = r.argmax()\n",
    "        l.append((maxi,r[maxi]))\n",
    "        r[maxi] = 0\n",
    "    return l\n",
    "\n",
    "\n",
    "def replaceZeroes(data):\n",
    "    #print(data)\n",
    "    data[data == 0] = 0.0001    \n",
    "    return data\n",
    "\n",
    "def replaceZeroeScalar(data):\n",
    "    if data == 0:\n",
    "        data = 0.0001   \n",
    "    \n",
    "    #print(data)\n",
    "    return data\n",
    "\n",
    "def IXT(px,pt_x,pt,m,n):\n",
    "    IXT = 0\n",
    "    for t in range(m):\n",
    "        #for x in range(n): #n len(x)\n",
    "            #print(replaceZeroes([1,3,0,7]))\n",
    "            a = px*pt_x[t,:]*np.log2(replaceZeroes(pt_x[t,:]/replaceZeroeScalar(pt[t])))\n",
    "            #print(a.sum())\n",
    "            IXT+=a.sum()\n",
    "    \n",
    "    print(\"IXT = \" + str(IXT))\n",
    "    return IXT\n",
    "\n",
    "def ITY_py_t(py_t,pxy,pt, m, k):\n",
    "    ITY = 0    \n",
    "    py = pxy.sum(axis=0)\n",
    "    for t in range(m):\n",
    "        #for y in range(k):\n",
    "            a = pt[t] * py_t[t,:]*np.log2(replaceZeroes(py_t[t,:]/replaceZeroes(py)))\n",
    "            ITY+=a.sum()   \n",
    "            #print(ITY)\n",
    "    \n",
    "    print(\"ITY = \" + str(ITY))\n",
    "    return ITY\n",
    "\n",
    "def plot_L(Func_L):\n",
    "    t = np.arange(0, len(Func_L), 1)\n",
    "    L = np.array(Func_L)\n",
    "    plt.plot(t, L)\n",
    "\n",
    "    plt.xlabel('iteration (k)')\n",
    "    plt.ylabel('L[p(t|x)]')\n",
    "    plt.title('I(X;T)-beta*I(T;Y)')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"test.png\")\n",
    "    plt.show()\n",
    "\n",
    "def f_sort(n, r):\n",
    "    l = []\n",
    "    for i in range(n):\n",
    "        maxi = r.argmax()\n",
    "        l.append(maxi)\n",
    "        r[maxi] = 0\n",
    "    return l\n",
    "\n",
    "def sent_weight(l,number_main_words):  #возвращает вес предложения\n",
    "    weights=[]\n",
    "    for word in l:\n",
    "        weights.append(mi_dict[new_word_dict[word]][0])\n",
    "    weights = np.array(weights)\n",
    "    weight = np.sum(weights[:number_main_words])\n",
    "    return weight\n",
    "\n",
    "\n",
    "def Q_foo(document):\n",
    "    return 2/3*len(parse_text(document)/sent_tokenize(document))\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_1(choice_document):\n",
    "    cmd = \"Select doc_text from Documents where id=?\"\n",
    "    cur_sql.execute(cmd,choice_document)\n",
    "    data = cur_sql.fetchone()[0]\n",
    "    cmd = \"Select claster_id from Doc_to_claster where doc_id=?\"\n",
    "    cur_sql.execute(cmd,choice_document)\n",
    "    cluster = cur_sql.fetchone()[0]\n",
    "    V = math.ceil(sent_tokenize(choice_document))\n",
    "    #MI_(num)\n",
    "    cmd = \"\"\"DECLARE @table_name sysname SELECT @table_name = 'MI_'+? \n",
    "    EXEC( 'select mi from ' + @table_name )\"\"\"\n",
    "    cur_sql.execute(cmd,cluster)\n",
    "    MI = cur_sql.fetchall()\n",
    "\n",
    "    #словарь слово ключ : индекс слова, значение : вес\n",
    "    mi_dict={}\n",
    "    for i, mi in enumerate(MI):\n",
    "        mi_dict[i] = mi\n",
    "\n",
    "    # переопределение словаря, теперь ключ: слово, значение :индекс\n",
    "    new_word_dict = {}\n",
    "    cmd = \"select id, name from Words\"\n",
    "    cur_sql.execute(cmd)\n",
    "    for i, word in tqdm(cur_sql.fetchall()):\n",
    "        new_word_dict[word] = i \n",
    "        \n",
    "        \n",
    "    sentences = sent_tokenize(data)\n",
    "    print(\"Number of sent = \"+str(len(sentences)))\n",
    "    sentence_weight=[]\n",
    "    for n,sent in enumerate(sentences):\n",
    "        parsed_text = parse_text(sent)\n",
    "        if len(parsed_text) > Q_foo(choice_document):      \n",
    "            sentence_weight.append(sent_weight(parsed_text,Q_foo(choice_document)))\n",
    "        else: \n",
    "            weight = 0\n",
    "            for word in parsed_text:\n",
    "                weight+=(mi_dict[new_word_dict[word]])\n",
    "            sentence_weight.append(weight)\n",
    "        \n",
    "            \n",
    "    sentence_weight = np.array(sentence_weight)\n",
    "    ss = f_sort(V,sentence_weight.copy())\n",
    "    ss = sorted(ss)\n",
    "    #for i in ss:\n",
    "    #    print(i,sentence_weight[i], sentences[i], len(word_tokenize(sentences[i])))   \n",
    "    return ss\n",
    "        \n",
    "        \n",
    "def summary_0(choice_document ):\n",
    "    ref=[]\n",
    "    cmd = \"Select doc_text from Documents where id=?\"\n",
    "    cur_sql.execute(cmd,choice_document)\n",
    "    data = cur_sql.fetchone()[0]\n",
    "    cmd = \"select * from Doc_to_claster_probability where doc_id = ?\"\n",
    "    cur_sql.execute(cmd,num)\n",
    "    V = math.ceil(sent_tokenize(choice_document))\n",
    "    distr=[]\n",
    "    for i in cur_sql.fetchone()[1:]:\n",
    "        distr.append(i)\n",
    "    distr = np.array(distr)\n",
    "    for cluster, prob in enumerate(distr):\n",
    "        cmd = \"\"\"DECLARE @table_name sysname SELECT @table_name = 'MI_'+? \n",
    "        EXEC( 'select mi from ' + @table_name ) \"\"\"\n",
    "        cur_sql.execute(cmd,cluster)\n",
    "        MI = cur_sql.fetchall()\n",
    "\n",
    "        #словарь слово ключ : индекс слова, значение : вес\n",
    "        mi_dict={}\n",
    "        for i, mi in enumerate(MI):\n",
    "            mi_dict[i] = mi\n",
    "\n",
    "        # переопределение словаря, теперь ключ: слово, значение :индекс\n",
    "        new_word_dict = {}\n",
    "        cmd = \"select id, name from Words\"\n",
    "        cur_sql.execute(cmd)\n",
    "        for i, word in tqdm(cur_sql.fetchall()):\n",
    "            new_word_dict[word] = i    \n",
    "        \n",
    "        sentences = sent_tokenize(data)\n",
    "        print(\"Number of sent = \"+str(len(sentences)))\n",
    "        sentence_weight=[]\n",
    "        for n,sent in enumerate(sentences):\n",
    "            parsed_text = parse_text(sent)\n",
    "            if len(parsed_text) > Q_foo(choice_document):      \n",
    "                sentence_weight.append(sent_weight(parsed_text,N))\n",
    "            else: \n",
    "                weight = 0\n",
    "                for word in parsed_text:\n",
    "                    weight+=(mi_dict[new_word_dict[word]])\n",
    "                sentence_weight.append(weight)\n",
    "        \n",
    "            \n",
    "        sentence_weight = np.array(sentence_weight)\n",
    "        ss = f_sort(math.ceil(V*distr),sentence_weight.copy())\n",
    "        ss = sorted(ss)\n",
    "        ref = list(set(ref + ss))\n",
    "        #for i in ss:\n",
    "        #    print(i,sentence_weight[i], sentences[i], len(word_tokenize(sentences[i])))   \n",
    "    return ref\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_bottleneck(beta, m):\n",
    "    start_time = time.clock()\n",
    "\n",
    " \n",
    "        \n",
    "    Func_L = []\n",
    "        \n",
    "    #словарь слов: ключ - номер индекса , значение -словао\n",
    "    word_dict = {}\n",
    "    for i, word in enumerate(index.dict.keys()):\n",
    "        word_dict[i] = word\n",
    "    docs_lens=[]\n",
    "    \"\"\"#общее число слов в коллекции\"\"\"\n",
    "    summa = 0\n",
    "    for doc in tqdm(x):\n",
    "        length = len(parse_text(doc))\n",
    "        docs_lens.append(length)\n",
    "        summa+= length\n",
    "        print(summa)\n",
    "\n",
    "    docs_lens = np.array(docs_lens)\n",
    "    px=docs_lens/summa\n",
    "    print(px.sum())\n",
    "    \"\"\"P(x,y)\"\"\"    \n",
    "\n",
    "    pxy = np.zeros((len(x), len(index.dict.keys())))\n",
    "    for i, word in enumerate(index.dict.keys()):\n",
    "        a = index.dict.get(word)\n",
    "        for doc_id , postings in a:\n",
    "            pxy[doc_id,i] = len(postings)/summa\n",
    "        \n",
    "    print(\"pxy + \")\n",
    "    print(pxy.sum())  \n",
    "    \"\"\"\n",
    "    print(pxy)\"\"\"     \n",
    "        \n",
    "    \"\"\"INITIALIZaTION\"\"\" \n",
    "\n",
    "    \"\"\"P(t|x)\"\"\"\n",
    "\n",
    "    pt_x = np.random.rand(m, len(x))\n",
    "\n",
    "    z = pt_x.sum(axis=0)\n",
    "    pt_x = pt_x/z\n",
    "    #print(pt_x)\n",
    "    \n",
    "\n",
    "\n",
    "    \"\"\"P(t)\"\"\"\n",
    "    pt = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        pt[i] = mult_vect(px,pt_x[i,:])\n",
    "    \n",
    "    \"\"\"P(y|t)\"\"\"\n",
    "    py_t = np.zeros((m,len(index.dict.keys())) )\n",
    "\n",
    "    for i in range(m):\n",
    "        for j in range(len(index.dict.keys())):\n",
    "            py_t[i,j] = mult_vect(pt_x[i,:],pxy[:,j])/pt[i]\n",
    "        \n",
    "    print(\"py_t +\")\n",
    "    print(py_t.sum(axis=1))\n",
    "    \"\"\"P(y|x)\"\"\"\n",
    "    py_x = np.zeros((len(index.dict.keys()),len(x)))\n",
    "\n",
    "\n",
    "    word_index=[]\n",
    "    for i, word in tqdm(enumerate(index.dict.keys())):\n",
    "        a = index.dict.get(word)\n",
    "        word_index.append((i,word))\n",
    "        for doc_id , postings in a:\n",
    "            py_x[i,doc_id] = len(postings)/docs_lens[doc_id]        \n",
    "    print(\"py_x + \")\n",
    "    print(py_x.sum(axis=0))\n",
    "      \n",
    "    \"\"\"LOOP\"\"\"\n",
    "    Div = np.zeros((len(x), m)) #init div   \n",
    "    pt_x_new = pt * np.exp(-beta * div_matr(Div, len(x),m,py_x,py_t)) \n",
    "\n",
    "    pt_x_new = pt_x_new.transpose()\n",
    "    z = foo_z(pt_x_new)\n",
    "    pt_x_new = pt_x_new/z\n",
    "\n",
    "\n",
    "    #init Djs\n",
    "    Djs = np.zeros(len(x))\n",
    "    Djs = djs_vect(Djs,len(x),pt_x,pt_x_new )\n",
    "\n",
    "    k = 0\n",
    "\n",
    "                                                                  \n",
    "    L = IXT(px,pt_x_new,pt,m, len(x)) - beta* ITY_py_t(py_t,pxy,pt, m, len(index.dict.keys()))\n",
    "\n",
    "    print(L)                                                                      \n",
    "    Func_L.append(L)\n",
    "    print(\"Djs\")\n",
    "    print(Djs.max())                                                                      \n",
    "    print (time.clock() - start_time, \"seconds\")\n",
    "\n",
    "    while Djs.max()>=e: \n",
    "        start_time = time.clock()\n",
    "        print(k, \"iteration\")\n",
    "        #переопрделяем \n",
    "        pt_x = pt_x_new \n",
    "   \n",
    "        pt = f_pt(pt,m,px,pt_x)\n",
    "    \n",
    "        #P(y|t)\n",
    "        py_t = f_py_t(py_t, m,len(index.dict.keys()),pt_x,pxy,pt) \n",
    "  \n",
    "        print(\"py_t +\")\n",
    "        print(py_t.sum(axis=1))             \n",
    "        #P(t|x) new\n",
    "        pt_x_new = pt * np.exp(-beta * div_matr(Div, len(x),m,py_x,py_t)) \n",
    "        \"\"\"print('pt_new')\n",
    "        print(pt_x_new)\"\"\"\n",
    "        pt_x_new = pt_x_new.transpose()\n",
    "        z = foo_z(pt_x_new)\n",
    "        pt_x_new = pt_x_new/z\n",
    "        print(\"pt_x_new\")\n",
    "       \n",
    "        Djs = djs_vect(Djs,len(x),pt_x,pt_x_new )\n",
    "        print(\"DJS\")\n",
    "        print(Djs.max())\n",
    "        k+=1\n",
    "        print(\"pt\")\n",
    "        print(pt)\n",
    "        L = IXT(px,pt_x_new,pt,m, len(x)) - beta* ITY_py_t(py_t,pxy,pt, m, len(index.dict.keys()))\n",
    "    \n",
    "        print(\"L = \"+str(L))                                                                      \n",
    "        Func_L.append(L)                                                                  \n",
    "        print (time.clock() - start_time, \"seconds\")\n",
    "    \n",
    "    # вектор с номерами кластеров, к которому документ предложит( вероятность больше всего)\n",
    "    vec = np.argmax(pt_x_new, axis=0)\n",
    "\n",
    "    #словарь: ключ - номер кластера, значение - номер документа\n",
    "    doc_clast = defaultdict(list)\n",
    "    for i,value in enumerate(vec):\n",
    "            doc_clast[value].append(i)\n",
    "    plot(Func_L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#beta\n",
    "beta = 2.4\n",
    "\n",
    "#число кластеров\n",
    "m = 5\n",
    "#covergence parameter \n",
    "e = 5e-04\n",
    "\n",
    "cmd = \"select doc_text from Documents \"\n",
    "cursor.execute(cmd)\n",
    "d=[]\n",
    "for t in cursor.fetchall()[:30]:      \n",
    "        d.append(t[0])\n",
    "x = np.array(d)       \n",
    "        \n",
    "analyzer = pymorphy2.MorphAnalyzer()  \n",
    "DocEntry = namedtuple('DocEntry',['doc_id','positions'])\n",
    "\"\"\"#Создание инвертированного индекса\"\"\"   \n",
    "index = InvertedIndex()\n",
    "\n",
    "for i, line in tqdm(enumerate(x)):\n",
    "        doc_id = i\n",
    "        text = line\n",
    "        index.add_document(text,doc_id)\n",
    "        \n",
    "information_bottleneck(beta, m)    \n",
    "\n",
    "# заполнение таблицы слов\n",
    "words = word_dict.items()\n",
    "for w in tqdm(words):\n",
    "    cmd = \"\"\"insert into Words (name) values (?) \"\"\"\n",
    "    cursor.execute(cmd, w[1])   \n",
    "    cursor.commit() \n",
    "    \n",
    "# заполнение таблицы кластеров\n",
    "for i in range(m):\n",
    "    cmd = \"\"\"insert into Clasters (name) values (?) \"\"\"\n",
    "    cursor.execute(cmd, ('name'+str(i)))\n",
    "    cursor.commit()\n",
    "# заполнение таблицы кластер-документ    \n",
    "for i in range(m):\n",
    "    l = doc_clast[i]\n",
    "    for number in l:\n",
    "        cmd = \"\"\"insert into  Doc_to_claster (doc_id,claster_id) values (?,?)\"\"\"\n",
    "        cursor.execute(cmd, (number, i))\n",
    "        cursor.commit()\n",
    "\n",
    "# распределение по кластерам\n",
    "for i,row in enumerate( pt_x.transpose()):\n",
    "    cmd = \"\"\"insert into Doc_to_claster_probability (doc_id,p0,p1,p2,p3,p4,p5,p6) values (?,?,?,?,?,?,?,?) \"\"\"\n",
    "    cursor.execute(cmd, (i,row[0],row[1],row[2],row[3],row[4],row[5],row[6]))\n",
    "    cursor.commit()\n",
    "\n",
    "    \n",
    "\n",
    "for cluster in m:\n",
    "    MI = mutual_information(cluster)\n",
    "    for i,mi in tqdm(enumerate(MI)):\n",
    "        cmd = \"\"\"DECLARE @table_name sysname SELECT @table_name = 'MI_'+?\n",
    "        EXEC( 'insert into ' + @table_name (word_id, mi) values (?,?) )\"\"\"\n",
    "        cursor.execute(cmd, (str(cluster), i,mi))  \n",
    "        cursor.commit()\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice_document = 5\n",
    "type_of_summary = 1 #1 если 1 кластер, 0 - если несколько кластеров\n",
    "if type_of_summary == 1:\n",
    "    summary_1(choice_document )\n",
    "else : \n",
    "    summary_0(choice_document )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
